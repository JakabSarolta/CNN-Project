{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51ba266",
   "metadata": {},
   "source": [
    "# Handwritten Number Recognition with Multiple Digits\n",
    "## 1, 2, 3-digit numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f666b90",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "from keras import Model\n",
    "from keras import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Normalization\n",
    "from keras.layers import Rescaling\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52590c6",
   "metadata": {},
   "source": [
    "# Reading the images from the 'Dataset' folder\n",
    "\n",
    "Sort by filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1aad13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"Dataset/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "def extract_number(filename):\n",
    "    return int(''.join(filter(str.isdigit, filename)))\n",
    "\n",
    "sorted_file_names = sorted(file_names, key=extract_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a5068",
   "metadata": {},
   "source": [
    "Convert images to grayscale and into numpy arrays.\n",
    "\n",
    "The images are added to the data list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4b74465",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in sorted_file_names:\n",
    "    # Opening the image file using PIL\n",
    "    image = Image.open(os.path.join(folder_path, filename))\n",
    "\n",
    "    image_data = np.array(image)\n",
    "    gray_image = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    data.append(255 - gray_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8dc183",
   "metadata": {},
   "source": [
    "Read all the labels from a csv file and convert them into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bf94b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = []\n",
    "\n",
    "with open('labels.csv', 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        for label in row:\n",
    "            labels_array.append(label)\n",
    "            \n",
    "labels = np.array(labels_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee08f79",
   "metadata": {},
   "source": [
    "# Explore the Data - Visualization\n",
    "\n",
    "Check how an image looks like after reading it from the input, and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fbfd87b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAADHCAYAAACEPVDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKNklEQVR4nO3dTYjVVR8H8P84kxMYMxWCJUZSw2iiFkVjBBVoIzlFL4siWky0cpG1yoUW0cqFKUJNVLSIEmqsKIWaspfZVEggFipMFL04mjK4kSSqaWbus3l4eM49N+dm/7lX5/f57L6Hn50TWHw5HP63pVKpVAoAIKw5zT4AANBcygAABKcMAEBwygAABKcMAEBwygAABKcMAEBwygAABKcMAEBwygAABKcMAEBwygAABKcMAEBwygAABKcMAEBwygAABKcMAEBwygAABKcMAEBwygAABKcMAEBwygAABNfW7ANAvSYmJmbsnz1nzpwzZoCZdPTo0WztiiuuaNj+/o8HAMEpAwAQnDIAAMG1VCqVSrMPQWP99ddfSf7xxx+zmdbW1mnXfvvtt2zmySefnHb/Wv/s9vb2JHd0dGQzQ0NDST516lQ2U+tdwdTU1LQzt9xyS5JXrlyZzbS1pU9slixZks2sX78+WwM417kZAIDglAEACE4ZAIDglAEACM4Dwlnu7bffztZeeumlJA8PD5e233333Zfk1atXZzPj4+PZ2vz585Pc399f2pmqnTx5Mlt74oknkvz666+Xtt+qVauSvG3btmymp6cnyXPnzi1tf4DpuBkAgOCUAQAIThkAgOCUAQAIzgPCWebQoUNJrvUlvQ0bNiT50ksvzWY2btyYrVV/ObD6y35FURQXXHBBks+Xh3DV/xkcOXIkm6n+952cnMxmduzYka19+umnSf7++++zmUcffTTJAwMDf39YgJK5GQCA4JQBAAhOGQCA4LwZmGVuvPHGJPf29mYzW7ZsadRxKIpiz549Sb733nuzmQceeCDJu3btmskjAeeYH374IVu7+uqrG7a/mwEACE4ZAIDglAEACE4ZAIDg2pp9AMp13XXXJbmrq6s5B+F/Xn311WYfAeCM3AwAQHDKAAAEpwwAQHDKAAAE5wuEzKhav2w4Z87s7aC7d+/O1vr7+5N88803ZzNvvfVWkjs6Oko9F3BuO3bsWLa2aNGihu0/e/+vDADURRkAgOCUAQAIzpsBSvXuu+8mefPmzdnMG2+8ka1df/31M3amsnz++edJ/uKLL7KZZ555Jlu79dZbk/zcc89lM9dcc82/OxzAv+BmAACCUwYAIDhlAACCUwYAIDi/WkipRkdHk3zJJZdkMzfccEO29vHHHye5t7e33IP9Q3/88Ue2Vv0Ycv/+/dnM4OBgtnb33XcnubW19V+eDqBcbgYAIDhlAACCUwYAIDgfHWJG/f7779nagw8+mK2NjY0lee/evdlMZ2dneQercvz48SSvW7cumzly5EiSh4eHs5nz4eNJANXcDABAcMoAAASnDABAcMoAAATnASENNzIykq2tWbMmyQsWLMhmPvjggyQvXLiwrv2q/4ofPnw4m7nnnnuSfOLEiWn3X716dV37A0zn22+/zdaWLl3asP3dDABAcMoAAASnDABAcMoAAATnASHnhKGhoSTfeeed2Ux3d3eS33nnnWxmxYoV2drJkyeTvGjRomymo6MjyT///HM2c+GFFybZrw8CZan1sHn58uUN29/NAAAEpwwAQHDKAAAE19bsA0BRFEVfX1+St27dms08/fTTSe7p6clmNmzYkK1t27YtydXvA4qiKAYHB5M8b968vz8sQMmmpqaaur+bAQAIThkAgOCUAQAIThkAgOB8dIjzxujoaJIfeeSRbGZ4eDhbmz9/fpK//vrrbKbWh4gAGuXgwYPZ2sqVKxu2v5sBAAhOGQCA4JQBAAjOR4c4b+zfvz/JBw4cqOvPVf+gUK2PDgFE5mYAAIJTBgAgOGUAAIJTBgAgOA8IOSdt3rw5W9uxY8e0f+6FF17I1l555ZUk9/f3ZzO7d++u/3AAJZucnGzq/m4GACA4ZQAAglMGACA4ZQAAgvOAkIabmJjI1gYGBpK8a9eubObKK69Mcq1Hf0uXLs3WPvrooyTv2bMnm3n//feTfNddd2UzALOVmwEACE4ZAIDglAEACK6lUqlUmn0IYunr68vWPvzwwyTfdttt2cybb76Z5Msvv7yu/UZGRpK8bNmybKa7u/uM5ymKorjqqqvq2g/gnzp16lS2dvHFFzdsfzcDABCcMgAAwSkDABCcMgAAwXlASKmmpqaSvHPnzmxm/fr12Vp7e3uSv/zyy2xm+fLlZ3Wm8fHxJG/fvj2bqf6VxFq/bPjaa6+d1f4A5zo3AwAQnDIAAMEpAwAQnDIAAMF5QEipfv311yR3dnZmM/PmzcvW9u3bl+QVK1aUe7D/c/z48WztjjvuSPLp06ezmU8++STJXV1d5R4MoEncDABAcMoAAASnDABAcG3NPgDnr4mJiWxtYGAgyW1t+V+xzz77LFubyTcC1RYuXJitXXbZZUk+dOhQNjM4OJjkp556qtyDATSJmwEACE4ZAIDglAEACE4ZAIDgfHSIs1brIWBvb2+Sb7/99mxm79692VpLS0t5BzsLBw8eTPK1116bzSxevDjJP/3000weCaBh3AwAQHDKAAAEpwwAQHA+OkTdTpw4keS1a9dmM9UfGerp6clmmv0+oJYlS5YkedOmTdnM9u3bk/zss89mMxs3biz3YEAI3333XbbW3d3dsP3dDABAcMoAAASnDABAcMoAAATnASF1O3r0aJKnpqaymZ07dyb5oYcemtEzlaW9vT3JtX7ZcHx8PMmjo6MzeiYgjrGxsWzNA0IAoGGUAQAIThkAgOCUAQAIzgNC6rZly5ZpZxYsWNCAk5Tvm2++SfJjjz2WzXR1dSX5+eefn8kjATSMmwEACE4ZAIDglAEACM6bAeo2OTnZ7COU4vDhw9na/fffP+2fu+mmm2biOADF3Llzm7q/mwEACE4ZAIDglAEACE4ZAIDgPCCkbi0tLdPOtLa2NuAk/8x7772X5Icffjibqf4Fxq1bt2Yzjz/+eLkHA/ivVatWNXV/NwMAEJwyAADBKQMAEFxLpVKpNPsQnB9GRkaSvGzZsmzm5ZdfTvLatWuzmcWLF5/V/r/88kuS+/r6spljx45la6dPn05yrbcPQ0NDSV6zZs3ZHBHgvORmAACCUwYAIDhlAACCUwYAIDgPCKnbn3/+meQXX3wxm9m0aVOSOzs7s5laDw/b2trOmIuiKPbt25fkiy66KJvp7e3N1qp/kXDdunXZDEBkbgYAIDhlAACCUwYAIDhlAACC84CQUn311VdJHhsby2bGx8enXat+rFgU+cPDZv/KF8Bs4WYAAIJTBgAgOGUAAILzZgAAgnMzAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADBKQMAEJwyAADB/QeSnfup7U2k4QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(data[1678], cmap=plt.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b98cd81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1678]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd84ae7",
   "metadata": {},
   "source": [
    "The labels and number are in correct pairs, as the above example shows.\n",
    "\n",
    "## Reshaping\n",
    "\n",
    "Because the sizes of the images differ, padding must be added so all images have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5142cd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "#select max width for the padding\n",
    "print(len(data[0][0]))\n",
    "max_width = len(data[0][0])\n",
    "#select max height for the padding\n",
    "print(len(data[0]))\n",
    "max_height = len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d276ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(data)):\n",
    "    #print(len(data[i][0]))\n",
    "    if max_width < len(data[i][0]):\n",
    "        max_width = len(data[i][0])\n",
    "    #print(len(data[i]))\n",
    "    if max_height < len(data[i]):\n",
    "        max_height = len(data[i])\n",
    "        \n",
    "print(max_width)\n",
    "print(max_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639ed6f",
   "metadata": {},
   "source": [
    "After finding the max width and length, we insert white pixels into the matrixes representing the images (rows or columns of 0s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82ab5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "padded_images = []\n",
    "for i in range(0,len(data)):\n",
    "    image = data[i]\n",
    "    padded_image = np.pad(image, ((0, max_height - image.shape[0]), (0, max_width - image.shape[1])), mode='constant')\n",
    "    padded_images.append(padded_image)\n",
    "\n",
    "# for i in range(len(data) - 1):\n",
    "#     print(\"Height: \", len(padded_images[i]))\n",
    "#     print(\"Width: \", len(padded_images[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b5fe5",
   "metadata": {},
   "source": [
    "Check randomly how an image looks like after inserting padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbe9f029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(data[1675], cmap=plt.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cac217dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to have type int instead of string\n",
    "labels = labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6030098",
   "metadata": {},
   "source": [
    "# Split the data - Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5435f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 8000\n",
      "Testing set size: 2000\n"
     ]
    }
   ],
   "source": [
    "# Split the data and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_images, labels, test_size=0.2)\n",
    "\n",
    "# Print the size of the training and testing sets\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Testing set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ae9dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of them into numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95966730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = x_train.reshape((8000, max_height*max_width))\n",
    "# X_test = x_test.reshape((2000, max_height*max_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b343ce34",
   "metadata": {},
   "source": [
    "Check an image from the training set and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47e95cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(X_train[0], cmap=plt.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "46de2856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at its class\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157ebd1",
   "metadata": {},
   "source": [
    "As the previous example shows, the labels and images are in order.\n",
    "\n",
    "# Building a Convolutional Neural Net\n",
    "\n",
    "Our model consists of several layers: \n",
    "* an input layer of size max width and max height, the depth being 1 for grayscale image\n",
    "* a rescaling layer: to work easily with the numbers in the matrices, all numbers should be between 0 and 1. Therefore, the greyscale values should be divided by 255\n",
    "* convolutional layers: they contain 64 or 128 filters/neurons; the chosen kernel size is 3x3 and the activation function is ReLU\n",
    "* max pooling layers: they decrease the input to half of its size, however, the depth doesn't change; the pooling window indicates the decrease height/2, width/2\n",
    "* flattening layer: creates a list from the matrices, multiplying all the dimensions\n",
    "* dense layer: they are the tipical neural net layers where all neurons are connected with every other neuron; one of the dense layers has 2048, the other one 1000 neurons\n",
    "* output layer: the last dense layer, which has 1000 neurons, one to predict each number from the interval [0-999]; it uses softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03135428",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(40, 108, 1))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048, activation=\"relu\")(x)\n",
    "outputs = Dense(1000, activation=\"softmax\")(x)\n",
    "convnet = Model(inputs, outputs)\n",
    "convnet.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ef3e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 40, 108, 1)]      0         \n",
      "                                                                 \n",
      " rescaling_2 (Rescaling)     (None, 40, 108, 1)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 38, 106, 64)       640       \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 19, 53, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 17, 51, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 8, 25, 128)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 23, 128)        147584    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 3, 11, 128)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 4224)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2048)              8652800   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1000)              2049000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,923,880\n",
      "Trainable params: 10,923,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc89bf1",
   "metadata": {},
   "source": [
    "The summary shows the output shape, where we can see how the 40 * 108 size matrix decreases gradually, and it shows the number of parameters in each layer. The number of these parameters can be calculated by taking into consideration the type of the layer and the shape of the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a88cfd",
   "metadata": {},
   "source": [
    "# Training the Neural Net\n",
    "\n",
    "The .fit function trains the neural net, using a batch size of 128 (chosen by us), and it calculates in 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e2553ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 64s 1s/step - loss: 4.9822 - accuracy: 0.0759 - val_loss: 4.3813 - val_accuracy: 0.1094\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 60s 1s/step - loss: 4.0286 - accuracy: 0.1311 - val_loss: 3.6745 - val_accuracy: 0.2069\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 61s 1s/step - loss: 2.9800 - accuracy: 0.3162 - val_loss: 2.7045 - val_accuracy: 0.3800\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 61s 1s/step - loss: 1.7992 - accuracy: 0.5494 - val_loss: 2.2302 - val_accuracy: 0.5025\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.9253 - accuracy: 0.7383 - val_loss: 2.1344 - val_accuracy: 0.5412\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.3560 - accuracy: 0.8961 - val_loss: 2.5711 - val_accuracy: 0.5663\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 63s 1s/step - loss: 0.1488 - accuracy: 0.9544 - val_loss: 2.7140 - val_accuracy: 0.5788\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.0753 - accuracy: 0.9797 - val_loss: 2.8385 - val_accuracy: 0.5981\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.0326 - accuracy: 0.9925 - val_loss: 2.9298 - val_accuracy: 0.6037\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.0301 - accuracy: 0.9919 - val_loss: 2.9632 - val_accuracy: 0.5925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd875fe4d0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet.fit(X_train, y_train, epochs=10, batch_size=128, \n",
    "            validation_split=0.2)\n",
    "            #callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5a7b1",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db12aaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 6s 96ms/step - loss: 2.8179 - accuracy: 0.6145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6144999861717224"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = convnet.evaluate(X_test, y_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016361c0",
   "metadata": {},
   "source": [
    "We can clearly see that the test accuracy is way lower than the training accuracy of the model.\n",
    "\n",
    "In the process of fine tuning and tring out different no of neurons, different kernels and different number of layers of different kind, we got vairous results.\n",
    "\n",
    "At first we had a 0.3 accuracy after testing.\n",
    "\n",
    "Then we changed the parameters and the convolutional layers to get a higher accuracy.\n",
    "\n",
    "The next try resulted in 0.44 accuracy.\n",
    "\n",
    "# Final Result\n",
    "\n",
    "The highest accuracy that we could achieve with this model was 0.62 so far. This result can be improved with fiddling around with the layers and parameters. We have built this model from scratch and it took a lot of time to run it after modifications on our laptops. Therefore, we have decided to stop at this accuracy rate, however small it is compared to a desired 80-90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5b48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
